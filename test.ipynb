{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Jun 24, 2023 1:26:31 AM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
      "WARNING: No Unicode mapping for a77 (2) in font IBTAJQ+ZapfDingbats\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tabula\n",
    "import numpy as np\n",
    "\n",
    "pdf_path = \"./pdfs/2008.pdf\"\n",
    "\n",
    "# Read PDF and extract tables from pages 26 to 41\n",
    "tables = tabula.read_pdf(pdf_path, pages='28-50', multiple_tables=True, stream=False)\n",
    "\n",
    "# Store cleaned rows and rows starting with 'nan'\n",
    "cleaned_rows = []\n",
    "nan_rows = []\n",
    "\n",
    "# Process tables\n",
    "for table in tables:\n",
    "    for row in table.itertuples(index=False):\n",
    "        if row[0] == np.nan:\n",
    "            nan_rows.append(row)\n",
    "        else:\n",
    "            cleaned_row = []\n",
    "            for cell in row:\n",
    "                if isinstance(cell, str):\n",
    "                    cleaned_cell = ' '.join(cell.split())\n",
    "                    cleaned_row.append(cleaned_cell)\n",
    "                else:\n",
    "                    cleaned_row.append(cell)\n",
    "            cleaned_rows.append(cleaned_row)\n",
    "\n",
    "# Write cleaned rows to a new file\n",
    "output_file = \"./2008.txt\"\n",
    "with open(output_file, 'w') as file:\n",
    "    for row in cleaned_rows:\n",
    "        file.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "# Append the rows starting with 'nan' to the new file\n",
    "with open(output_file, 'a') as file:\n",
    "    file.write(\"#### Note: These are the rows starting with 'nan':\\n\")\n",
    "    for row in nan_rows:\n",
    "        cleaned_row = []\n",
    "        for cell in row:\n",
    "            if isinstance(cell, str):\n",
    "                cleaned_cell = ' '.join(cell.split())\n",
    "                cleaned_row.append(cleaned_cell)\n",
    "            else:\n",
    "                cleaned_row.append(cell)\n",
    "        file.write('\\t'.join(map(str, cleaned_row)) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tabula\n",
    "\n",
    "pdf_path = \"./pdfs/2005.pdf\"\n",
    "\n",
    "# Read PDF and extract tables from pages 26 to 41\n",
    "tables = tabula.read_pdf(pdf_path, pages='22-34', multiple_tables=True, stream=False)\n",
    "\n",
    "# Save tables to a text file\n",
    "output_file = \"./2005.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for table in tables:\n",
    "        for row in table.itertuples(index=False):\n",
    "            cleaned_row = []\n",
    "            for cell in row:\n",
    "                if isinstance(cell, str):\n",
    "                    cleaned_cell = ' '.join(cell.split())\n",
    "                    cleaned_row.append(cleaned_cell)\n",
    "                else:\n",
    "                    cleaned_row.append(cell)\n",
    "            csv_contents = '\\t'.join(map(str, cleaned_row)) + '\\n'\n",
    "            \n",
    "            # Apply regex pattern to each row\n",
    "            pattern = r'^(.*?),\"([^\"]+)\",.*?,((?:\\d+\\.?\\d*|),(?:\\d+\\.?\\d*|),(?:\\d+\\.?\\d*|),)'\n",
    "            matches = re.findall(pattern, csv_contents, re.MULTILINE)\n",
    "            \n",
    "            if matches:\n",
    "                # If matches are found, write them to the file\n",
    "                for match in matches:\n",
    "                    file.write('\\t'.join(match) + '\\n')\n",
    "            else:\n",
    "                # If no matches are found, write the original row to the file\n",
    "                file.write(csv_contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "###2002-2004\n",
    "def convert_to_tsv(content):\n",
    "    # Pattern for the headers\n",
    "    header_pattern = r\"Employer Name City State HRC Rating\"\n",
    "    \n",
    "    # Pattern for the lines with company data\n",
    "    company_pattern = r\"^(.*\\S)\\s+([A-Za-z\\s-]+?)\\s+([A-Z]{2})\\s+(\\d+)$\"\n",
    "    \n",
    "    tsv_lines = []\n",
    "    removed_lines = []\n",
    "    \n",
    "    for line in content.split(\"\\n\"):\n",
    "        if re.match(header_pattern, line):\n",
    "            # Skip headers\n",
    "            continue\n",
    "        \n",
    "        match = re.match(company_pattern, line)\n",
    "        if match:\n",
    "            tsv_line = \"\\t\".join(match.groups())\n",
    "            tsv_lines.append(tsv_line)\n",
    "        else:\n",
    "            # Collect lines that did not match\n",
    "            removed_lines.append(line)\n",
    "    \n",
    "    tsv_content = \"\\n\".join(tsv_lines)\n",
    "    \n",
    "    # Add removed lines to the end with a special note\n",
    "    if removed_lines:\n",
    "        tsv_content += \"\\n\\n##Some rows did not match the pattern:\\n\"\n",
    "        tsv_content += \"\\n\".join(removed_lines)\n",
    "    \n",
    "    return tsv_content\n",
    "\n",
    "# Read from the input file\n",
    "with open(\"copy_paste/2005.tsv\", \"r\", encoding=\"utf-8\" ) as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Convert the content to TSV format\n",
    "tsv_content = convert_to_tsv(content)\n",
    "\n",
    "# Write to the output file\n",
    "with open(\"output.tsv\", \"w\") as file:\n",
    "    file.write(tsv_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"copy_paste/2005.tsv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.readlines()\n",
    "\n",
    "cleaned_content = []\n",
    "\n",
    "for line in content:\n",
    "    # Remove individual 3's from the line\n",
    "    line = re.sub(r'\\b3\\b', '', line)\n",
    "\n",
    "    # Extract the name and the first three numbers after the string\n",
    "    match = re.search(r'^(.*?)((?:\\s+\\d+){3})', line)\n",
    "    if match:\n",
    "        name = match.group(1).strip()\n",
    "        numbers = match.group(2).split()\n",
    "        if len(numbers) == 3:\n",
    "            cleaned_line = [name] + numbers\n",
    "        else:\n",
    "            cleaned_line = [name, None] + numbers\n",
    "        cleaned_content.append(cleaned_line)\n",
    "\n",
    "# Fix the city names in the cleaned content\n",
    "fixed_content = []\n",
    "for line in cleaned_content:\n",
    "    fixed_line = re.sub(r'\\s+([A-Z]{2})\\s+(\\d+)$', r'\\t\\1\\t\\2', line[0])\n",
    "    fixed_content.append(fixed_line + '\\t' + '\\t'.join(str(item) for item in line[1:]))\n",
    "\n",
    "# Write the cleaned and fixed content to output.tsv\n",
    "with open(\"output.tsv\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for line in fixed_content:\n",
    "        output_file.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "state_data = states\n",
    "\n",
    "# Create a dictionary to map state names to abbreviations\n",
    "state_abbrev_map = {state[\"state\"].lower(): state[\"abbrev\"] for state in state_data}\n",
    "\n",
    "# Define the regex pattern\n",
    "pattern = r\"^(.+?)\\s+((?:[A-Za-z\\s]+?,\\s)?[A-Za-z\\s]+?)\\t(\\d+)\\t(\\d+)\\t(\\d+)$\"\n",
    "\n",
    "# Read input data from file\n",
    "with open(\"output.tsv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    input_data = file.readlines()\n",
    "\n",
    "output = \"firm\\tcity\\tstate\\tscore1\\tscore2\\tscore3\\n\"\n",
    "\n",
    "for line in input_data:\n",
    "    match = re.match(pattern, line)\n",
    "    if match:\n",
    "        firm = match.group(1)\n",
    "        city = match.group(2)\n",
    "        score1 = match.group(3)\n",
    "        score2 = match.group(4)\n",
    "        score3 = match.group(5)\n",
    "\n",
    "        # Find the state abbreviation using the state name\n",
    "        state_name = city.split(\",\")[-1].strip().lower()\n",
    "        state_abbrev = state_abbrev_map.get(state_name, \"N/A\")\n",
    "\n",
    "        output += f\"{firm}\\t{city}\\t{state_abbrev}\\t{score1}\\t{score2}\\t{score3}\\n\"\n",
    "\n",
    "# Write output to file\n",
    "with open(\"output_2.tsv\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evan\\AppData\\Local\\Temp\\ipykernel_2472\\2798094446.py:5: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(\"2006.tsv\", sep=\"\\t\", header=None, error_bad_lines=False)\n",
      "b'Skipping line 5: expected 17 fields, saw 18\\nSkipping line 7: expected 17 fields, saw 18\\nSkipping line 8: expected 17 fields, saw 18\\nSkipping line 9: expected 17 fields, saw 18\\nSkipping line 11: expected 17 fields, saw 18\\nSkipping line 12: expected 17 fields, saw 18\\nSkipping line 13: expected 17 fields, saw 18\\nSkipping line 14: expected 17 fields, saw 19\\nSkipping line 16: expected 17 fields, saw 18\\nSkipping line 18: expected 17 fields, saw 18\\nSkipping line 20: expected 17 fields, saw 18\\nSkipping line 21: expected 17 fields, saw 18\\nSkipping line 22: expected 17 fields, saw 18\\nSkipping line 23: expected 17 fields, saw 18\\nSkipping line 25: expected 17 fields, saw 18\\nSkipping line 27: expected 17 fields, saw 18\\nSkipping line 28: expected 17 fields, saw 18\\nSkipping line 30: expected 17 fields, saw 18\\nSkipping line 32: expected 17 fields, saw 18\\nSkipping line 34: expected 17 fields, saw 18\\nSkipping line 35: expected 17 fields, saw 18\\nSkipping line 36: expected 17 fields, saw 18\\nSkipping line 38: expected 17 fields, saw 18\\nSkipping line 40: expected 17 fields, saw 18\\nSkipping line 41: expected 17 fields, saw 18\\nSkipping line 42: expected 17 fields, saw 18\\nSkipping line 43: expected 17 fields, saw 18\\nSkipping line 44: expected 17 fields, saw 18\\nSkipping line 45: expected 17 fields, saw 18\\nSkipping line 46: expected 17 fields, saw 18\\nSkipping line 48: expected 17 fields, saw 18\\nSkipping line 49: expected 17 fields, saw 18\\nSkipping line 50: expected 17 fields, saw 18\\nSkipping line 52: expected 17 fields, saw 18\\nSkipping line 54: expected 17 fields, saw 18\\nSkipping line 55: expected 17 fields, saw 18\\nSkipping line 56: expected 17 fields, saw 18\\nSkipping line 57: expected 17 fields, saw 19\\nSkipping line 58: expected 17 fields, saw 19\\nSkipping line 59: expected 17 fields, saw 18\\nSkipping line 60: expected 17 fields, saw 19\\nSkipping line 62: expected 17 fields, saw 19\\nSkipping line 64: expected 17 fields, saw 19\\nSkipping line 65: expected 17 fields, saw 18\\nSkipping line 66: expected 17 fields, saw 19\\nSkipping line 67: expected 17 fields, saw 18\\nSkipping line 68: expected 17 fields, saw 19\\nSkipping line 69: expected 17 fields, saw 19\\nSkipping line 70: expected 17 fields, saw 18\\nSkipping line 71: expected 17 fields, saw 19\\nSkipping line 73: expected 17 fields, saw 19\\nSkipping line 75: expected 17 fields, saw 19\\nSkipping line 77: expected 17 fields, saw 19\\nSkipping line 78: expected 17 fields, saw 19\\nSkipping line 79: expected 17 fields, saw 18\\nSkipping line 80: expected 17 fields, saw 19\\nSkipping line 82: expected 17 fields, saw 19\\nSkipping line 83: expected 17 fields, saw 19\\nSkipping line 84: expected 17 fields, saw 19\\nSkipping line 86: expected 17 fields, saw 19\\nSkipping line 88: expected 17 fields, saw 19\\nSkipping line 90: expected 17 fields, saw 19\\nSkipping line 92: expected 17 fields, saw 19\\nSkipping line 94: expected 17 fields, saw 19\\nSkipping line 96: expected 17 fields, saw 19\\nSkipping line 97: expected 17 fields, saw 18\\nSkipping line 98: expected 17 fields, saw 19\\nSkipping line 100: expected 17 fields, saw 19\\nSkipping line 102: expected 17 fields, saw 19\\nSkipping line 104: expected 17 fields, saw 19\\nSkipping line 106: expected 17 fields, saw 19\\nSkipping line 108: expected 17 fields, saw 19\\nSkipping line 109: expected 17 fields, saw 19\\nSkipping line 110: expected 17 fields, saw 19\\nSkipping line 112: expected 17 fields, saw 19\\nSkipping line 114: expected 17 fields, saw 19\\nSkipping line 115: expected 17 fields, saw 19\\nSkipping line 116: expected 17 fields, saw 19\\nSkipping line 117: expected 17 fields, saw 18\\nSkipping line 118: expected 17 fields, saw 19\\nSkipping line 121: expected 17 fields, saw 19\\nSkipping line 122: expected 17 fields, saw 18\\nSkipping line 123: expected 17 fields, saw 19\\nSkipping line 125: expected 17 fields, saw 19\\nSkipping line 126: expected 17 fields, saw 19\\nSkipping line 127: expected 17 fields, saw 19\\nSkipping line 128: expected 17 fields, saw 18\\nSkipping line 129: expected 17 fields, saw 18\\nSkipping line 130: expected 17 fields, saw 18\\nSkipping line 131: expected 17 fields, saw 20\\nSkipping line 133: expected 17 fields, saw 20\\nSkipping line 135: expected 17 fields, saw 18\\nSkipping line 137: expected 17 fields, saw 18\\nSkipping line 139: expected 17 fields, saw 19\\nSkipping line 140: expected 17 fields, saw 19\\nSkipping line 141: expected 17 fields, saw 18\\nSkipping line 142: expected 17 fields, saw 18\\nSkipping line 143: expected 17 fields, saw 19\\nSkipping line 145: expected 17 fields, saw 18\\nSkipping line 146: expected 17 fields, saw 18\\nSkipping line 147: expected 17 fields, saw 18\\nSkipping line 149: expected 17 fields, saw 18\\nSkipping line 151: expected 17 fields, saw 19\\nSkipping line 152: expected 17 fields, saw 18\\nSkipping line 153: expected 17 fields, saw 18\\nSkipping line 155: expected 17 fields, saw 18\\nSkipping line 157: expected 17 fields, saw 18\\nSkipping line 159: expected 17 fields, saw 18\\nSkipping line 160: expected 17 fields, saw 18\\nSkipping line 162: expected 17 fields, saw 20\\nSkipping line 164: expected 17 fields, saw 18\\nSkipping line 166: expected 17 fields, saw 18\\nSkipping line 168: expected 17 fields, saw 20\\nSkipping line 169: expected 17 fields, saw 18\\nSkipping line 171: expected 17 fields, saw 18\\nSkipping line 172: expected 17 fields, saw 18\\nSkipping line 173: expected 17 fields, saw 20\\nSkipping line 175: expected 17 fields, saw 18\\nSkipping line 177: expected 17 fields, saw 18\\nSkipping line 179: expected 17 fields, saw 18\\nSkipping line 180: expected 17 fields, saw 18\\nSkipping line 181: expected 17 fields, saw 18\\nSkipping line 182: expected 17 fields, saw 18\\nSkipping line 183: expected 17 fields, saw 18\\nSkipping line 185: expected 17 fields, saw 31\\nSkipping line 186: expected 17 fields, saw 18\\nSkipping line 187: expected 17 fields, saw 18\\nSkipping line 189: expected 17 fields, saw 18\\nSkipping line 191: expected 17 fields, saw 18\\nSkipping line 193: expected 17 fields, saw 18\\nSkipping line 195: expected 17 fields, saw 18\\nSkipping line 197: expected 17 fields, saw 18\\nSkipping line 198: expected 17 fields, saw 18\\nSkipping line 200: expected 17 fields, saw 18\\nSkipping line 201: expected 17 fields, saw 18\\nSkipping line 202: expected 17 fields, saw 18\\nSkipping line 204: expected 17 fields, saw 18\\nSkipping line 206: expected 17 fields, saw 18\\nSkipping line 207: expected 17 fields, saw 19\\nSkipping line 209: expected 17 fields, saw 19\\nSkipping line 211: expected 17 fields, saw 19\\nSkipping line 213: expected 17 fields, saw 19\\nSkipping line 215: expected 17 fields, saw 19\\nSkipping line 216: expected 17 fields, saw 19\\nSkipping line 218: expected 17 fields, saw 19\\nSkipping line 221: expected 17 fields, saw 19\\nSkipping line 222: expected 17 fields, saw 18\\nSkipping line 223: expected 17 fields, saw 19\\nSkipping line 224: expected 17 fields, saw 18\\nSkipping line 225: expected 17 fields, saw 18\\nSkipping line 226: expected 17 fields, saw 18\\nSkipping line 227: expected 17 fields, saw 19\\nSkipping line 229: expected 17 fields, saw 19\\nSkipping line 230: expected 17 fields, saw 19\\nSkipping line 231: expected 17 fields, saw 19\\nSkipping line 233: expected 17 fields, saw 19\\nSkipping line 234: expected 17 fields, saw 19\\nSkipping line 235: expected 17 fields, saw 18\\nSkipping line 236: expected 17 fields, saw 19\\nSkipping line 238: expected 17 fields, saw 19\\nSkipping line 240: expected 17 fields, saw 19\\nSkipping line 242: expected 17 fields, saw 19\\nSkipping line 244: expected 17 fields, saw 19\\nSkipping line 246: expected 17 fields, saw 19\\nSkipping line 247: expected 17 fields, saw 19\\nSkipping line 249: expected 17 fields, saw 19\\nSkipping line 250: expected 17 fields, saw 19\\nSkipping line 252: expected 17 fields, saw 19\\nSkipping line 254: expected 17 fields, saw 19\\nSkipping line 257: expected 17 fields, saw 19\\nSkipping line 258: expected 17 fields, saw 19\\nSkipping line 260: expected 17 fields, saw 19\\nSkipping line 262: expected 17 fields, saw 19\\nSkipping line 264: expected 17 fields, saw 19\\nSkipping line 265: expected 17 fields, saw 19\\nSkipping line 267: expected 17 fields, saw 19\\nSkipping line 269: expected 17 fields, saw 19\\nSkipping line 271: expected 17 fields, saw 19\\nSkipping line 272: expected 17 fields, saw 23\\nSkipping line 273: expected 17 fields, saw 19\\nSkipping line 274: expected 17 fields, saw 19\\nSkipping line 275: expected 17 fields, saw 18\\nSkipping line 276: expected 17 fields, saw 20\\nSkipping line 295: expected 17 fields, saw 18\\nSkipping line 296: expected 17 fields, saw 19\\nSkipping line 298: expected 17 fields, saw 19\\nSkipping line 304: expected 17 fields, saw 19\\nSkipping line 305: expected 17 fields, saw 19\\nSkipping line 307: expected 17 fields, saw 18\\nSkipping line 310: expected 17 fields, saw 19\\nSkipping line 314: expected 17 fields, saw 19\\nSkipping line 318: expected 17 fields, saw 18\\nSkipping line 321: expected 17 fields, saw 19\\nSkipping line 324: expected 17 fields, saw 18\\nSkipping line 328: expected 17 fields, saw 18\\nSkipping line 336: expected 17 fields, saw 18\\nSkipping line 339: expected 17 fields, saw 18\\nSkipping line 344: expected 17 fields, saw 18\\nSkipping line 348: expected 17 fields, saw 18\\nSkipping line 352: expected 17 fields, saw 18\\nSkipping line 354: expected 17 fields, saw 18\\nSkipping line 356: expected 17 fields, saw 18\\nSkipping line 357: expected 17 fields, saw 18\\nSkipping line 358: expected 17 fields, saw 18\\nSkipping line 360: expected 17 fields, saw 18\\nSkipping line 361: expected 17 fields, saw 18\\nSkipping line 363: expected 17 fields, saw 18\\nSkipping line 364: expected 17 fields, saw 18\\nSkipping line 365: expected 17 fields, saw 18\\nSkipping line 366: expected 17 fields, saw 18\\nSkipping line 367: expected 17 fields, saw 18\\nSkipping line 368: expected 17 fields, saw 18\\nSkipping line 369: expected 17 fields, saw 18\\nSkipping line 370: expected 17 fields, saw 18\\nSkipping line 371: expected 17 fields, saw 18\\nSkipping line 372: expected 17 fields, saw 18\\nSkipping line 373: expected 17 fields, saw 18\\nSkipping line 375: expected 17 fields, saw 18\\nSkipping line 376: expected 17 fields, saw 18\\nSkipping line 377: expected 17 fields, saw 18\\nSkipping line 379: expected 17 fields, saw 18\\nSkipping line 380: expected 17 fields, saw 18\\nSkipping line 382: expected 17 fields, saw 18\\nSkipping line 383: expected 17 fields, saw 18\\nSkipping line 385: expected 17 fields, saw 18\\nSkipping line 387: expected 17 fields, saw 18\\nSkipping line 390: expected 17 fields, saw 18\\nSkipping line 392: expected 17 fields, saw 18\\nSkipping line 394: expected 17 fields, saw 18\\nSkipping line 396: expected 17 fields, saw 18\\nSkipping line 397: expected 17 fields, saw 18\\nSkipping line 398: expected 17 fields, saw 18\\nSkipping line 400: expected 17 fields, saw 18\\nSkipping line 402: expected 17 fields, saw 18\\nSkipping line 403: expected 17 fields, saw 18\\nSkipping line 404: expected 17 fields, saw 18\\nSkipping line 406: expected 17 fields, saw 18\\nSkipping line 408: expected 17 fields, saw 18\\nSkipping line 410: expected 17 fields, saw 18\\nSkipping line 411: expected 17 fields, saw 18\\nSkipping line 413: expected 17 fields, saw 18\\nSkipping line 415: expected 17 fields, saw 18\\nSkipping line 417: expected 17 fields, saw 18\\nSkipping line 419: expected 17 fields, saw 18\\nSkipping line 421: expected 17 fields, saw 18\\nSkipping line 422: expected 17 fields, saw 18\\nSkipping line 423: expected 17 fields, saw 18\\nSkipping line 424: expected 17 fields, saw 18\\nSkipping line 425: expected 17 fields, saw 18\\nSkipping line 426: expected 17 fields, saw 19\\nSkipping line 427: expected 17 fields, saw 18\\nSkipping line 428: expected 17 fields, saw 18\\nSkipping line 430: expected 17 fields, saw 18\\nSkipping line 431: expected 17 fields, saw 18\\nSkipping line 433: expected 17 fields, saw 18\\nSkipping line 435: expected 17 fields, saw 18\\nSkipping line 437: expected 17 fields, saw 18\\nSkipping line 438: expected 17 fields, saw 18\\nSkipping line 439: expected 17 fields, saw 20\\nSkipping line 440: expected 17 fields, saw 18\\nSkipping line 442: expected 17 fields, saw 18\\nSkipping line 443: expected 17 fields, saw 18\\nSkipping line 444: expected 17 fields, saw 18\\nSkipping line 445: expected 17 fields, saw 18\\nSkipping line 446: expected 17 fields, saw 18\\nSkipping line 448: expected 17 fields, saw 18\\nSkipping line 449: expected 17 fields, saw 18\\nSkipping line 450: expected 17 fields, saw 18\\nSkipping line 451: expected 17 fields, saw 18\\nSkipping line 452: expected 17 fields, saw 18\\nSkipping line 453: expected 17 fields, saw 18\\nSkipping line 454: expected 17 fields, saw 18\\nSkipping line 456: expected 17 fields, saw 19\\nSkipping line 457: expected 17 fields, saw 18\\nSkipping line 458: expected 17 fields, saw 20\\nSkipping line 459: expected 17 fields, saw 18\\nSkipping line 460: expected 17 fields, saw 18\\nSkipping line 461: expected 17 fields, saw 20\\nSkipping line 463: expected 17 fields, saw 18\\nSkipping line 464: expected 17 fields, saw 18\\nSkipping line 466: expected 17 fields, saw 18\\nSkipping line 468: expected 17 fields, saw 18\\nSkipping line 469: expected 17 fields, saw 18\\nSkipping line 470: expected 17 fields, saw 18\\nSkipping line 472: expected 17 fields, saw 18\\nSkipping line 474: expected 17 fields, saw 18\\nSkipping line 476: expected 17 fields, saw 18\\nSkipping line 478: expected 17 fields, saw 18\\nSkipping line 479: expected 17 fields, saw 18\\nSkipping line 480: expected 17 fields, saw 18\\nSkipping line 481: expected 17 fields, saw 18\\nSkipping line 482: expected 17 fields, saw 18\\nSkipping line 483: expected 17 fields, saw 18\\nSkipping line 485: expected 17 fields, saw 18\\nSkipping line 493: expected 17 fields, saw 19\\nSkipping line 495: expected 17 fields, saw 18\\nSkipping line 496: expected 17 fields, saw 18\\nSkipping line 500: expected 17 fields, saw 18\\nSkipping line 501: expected 17 fields, saw 18\\nSkipping line 512: expected 17 fields, saw 18\\nSkipping line 514: expected 17 fields, saw 18\\nSkipping line 517: expected 17 fields, saw 18\\nSkipping line 523: expected 17 fields, saw 18\\nSkipping line 525: expected 17 fields, saw 18\\nSkipping line 528: expected 17 fields, saw 18\\nSkipping line 530: expected 17 fields, saw 18\\nSkipping line 532: expected 17 fields, saw 18\\nSkipping line 535: expected 17 fields, saw 18\\nSkipping line 539: expected 17 fields, saw 18\\nSkipping line 545: expected 17 fields, saw 18\\nSkipping line 546: expected 17 fields, saw 19\\nSkipping line 550: expected 17 fields, saw 18\\nSkipping line 551: expected 17 fields, saw 18\\nSkipping line 552: expected 17 fields, saw 18\\nSkipping line 554: expected 17 fields, saw 18\\nSkipping line 556: expected 17 fields, saw 18\\nSkipping line 558: expected 17 fields, saw 18\\nSkipping line 561: expected 17 fields, saw 18\\nSkipping line 563: expected 17 fields, saw 18\\nSkipping line 564: expected 17 fields, saw 18\\nSkipping line 583: expected 17 fields, saw 18\\nSkipping line 584: expected 17 fields, saw 18\\nSkipping line 591: expected 17 fields, saw 18\\nSkipping line 592: expected 17 fields, saw 18\\nSkipping line 597: expected 17 fields, saw 18\\nSkipping line 598: expected 17 fields, saw 18\\n'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the file into a DataFrame\n",
    "df = pd.read_csv(\"2008.tsv\", sep=\"\\t\", header=None, error_bad_lines=False)\n",
    "\n",
    "# Select only the first 7 columns and drop the rest\n",
    "df = df.iloc[:, :7]\n",
    "\n",
    "# Assigning column names to the selected 7 columns\n",
    "df.columns = [\"COMPANY\", \"CITY\", \"STATE\", \"FORTUNE\", \"FORBES\", \"AMLAW\", \"CEI\"]\n",
    "\n",
    "# Convert the columns 3 to 6 to string\n",
    "df.iloc[:, 3:7] = df.iloc[:, 3:7].astype(str)\n",
    "\n",
    "# Normalize values in the columns 3 to 6\n",
    "df.iloc[:, 3:7] = df.iloc[:, 3:7].replace({'\\.0': ''}, regex=True)\n",
    "\n",
    "# Replace '5' and '15' with NaN\n",
    "df.iloc[:, 3:7] = df.iloc[:, 3:7].replace({'5': np.nan, '15': np.nan})\n",
    "\n",
    "# Replace 'nan' string to actual NaN value\n",
    "df = df.replace({'nan': np.nan})\n",
    "\n",
    "# Drop duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save the modified DataFrame to a new file, replacing NaN values with 'N/A'\n",
    "df.to_csv(\"modified_2007.tsv\", sep=\"\\t\", na_rep='N/A', header=True, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_csv_manual(filename, delimiter='\\t', num_fields=17):\n",
    "    lines_to_skip = []\n",
    "    data = []\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        for line_num, line in enumerate(file):\n",
    "            fields = line.strip().split(delimiter)\n",
    "            if len(fields) != num_fields:\n",
    "                print(f\"Skipping line {line_num + 1}: expected {num_fields} fields, saw {len(fields)}\")\n",
    "                lines_to_skip.append(line_num + 1)\n",
    "            else:\n",
    "                data.append(fields)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df, lines_to_skip\n",
    "\n",
    "# Usage\n",
    "filename = \"2006.tsv\"\n",
    "df, skipped_lines = read_csv_manual(filename)\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "df.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(\"Skipped lines:\", skipped_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file for reading\n",
    "with open('2006.tsv', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Use a dictionary to store firm names and their longest rows\n",
    "unique_firms = {}\n",
    "for line in lines:\n",
    "    row = line.strip().split('\\t')\n",
    "    firm_name = row[0]\n",
    "    row_length = len(row)\n",
    "    \n",
    "    if firm_name not in unique_firms or row_length > len(unique_firms[firm_name]):\n",
    "        unique_firms[firm_name] = row\n",
    "\n",
    "# Write the longest rows to the output file\n",
    "with open('output.tsv', 'w') as file:\n",
    "    for row in unique_firms.values():\n",
    "        file.write('\\t'.join(row) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the input file for reading\n",
    "with open('output.tsv', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Extract the first seven values from each row\n",
    "values = [line.strip().split('\\t')[:7] for line in lines]\n",
    "\n",
    "# Write the extracted values to the output2 file\n",
    "with open('output2.tsv', 'w') as file:\n",
    "    for row in values:\n",
    "        file.write('\\t'.join(row) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data has been written to output2_shifted.tsv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_file = 'output2.tsv'\n",
    "output_file = 'output2_shifted.tsv'\n",
    "\n",
    "# Read the input TSV file\n",
    "rows = []\n",
    "with open(input_file, 'r', newline='') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    rows = list(reader)\n",
    "\n",
    "# Determine the maximum length of rows\n",
    "max_length = max(len(row) for row in rows)\n",
    "\n",
    "# Adjust the rows to have a length of 7 and shift the last number\n",
    "for row in rows:\n",
    "    while len(row) < 7:\n",
    "        row.insert(3, 'N/A')\n",
    "    last_number = None\n",
    "    for i in range(len(row) - 1, -1, -1):\n",
    "        if row[i] != '':\n",
    "            last_number = row[i]\n",
    "            row[i] = 'N/A'\n",
    "            for j in range(i - 1, -1, -1):\n",
    "                if row[j] == '':\n",
    "                    row[j] = 'N/A'\n",
    "                else:\n",
    "                    break\n",
    "            break\n",
    "    if last_number is not None:\n",
    "        row[6] = last_number\n",
    "\n",
    "# Write the updated data to a new TSV file\n",
    "with open(output_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter='\\t')\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Updated data has been written to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "# Read 2006.tsv and create a dictionary of company names and cities\n",
    "company_city_2006 = {}\n",
    "with open('./copy_paste/2006.tsv', 'r') as file_2006:\n",
    "    reader_2006 = csv.reader(file_2006, delimiter='\\t')\n",
    "    next(reader_2006)  # Skip the header row\n",
    "    for row in reader_2006:\n",
    "        if len(row) >= 2:\n",
    "            company = row[0]\n",
    "            city_state = row[1].replace(',', '\\t')\n",
    "            company_city_2006[company] = city_state\n",
    "\n",
    "# Open 2008.tsv and output.tsv files\n",
    "with open('./2008.tsv', 'r') as file_2008, open('./output33.tsv', 'w') as outfile:\n",
    "    reader_2008 = csv.reader(file_2008, delimiter='\\t')\n",
    "    writer = csv.writer(outfile, delimiter='\\t')\n",
    "\n",
    "    # Regex pattern to match company, city and state in the string\n",
    "    pattern = re.compile(r'^(.*?)\\t(.*?),\\s*(\\w{2})\\t')\n",
    "\n",
    "    # Iterate over 2008.tsv rows\n",
    "    for row in reader_2008:\n",
    "        # Only process rows with at least 2 columns\n",
    "        if len(row) >= 3:\n",
    "            match = pattern.match(row[0])\n",
    "            if match:\n",
    "                company, city, state = match.groups()\n",
    "                if company in company_city_2006:\n",
    "                    city_state_2006 = company_city_2006[company]\n",
    "                    # Replace the city and state with the values from 2006.tsv\n",
    "                    row[0] = pattern.sub(f'{company}\\t{city_state_2006}\\t', row[0])\n",
    "                else:\n",
    "                    row[0] = '\\n' + row[0]\n",
    "\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "filename = '2008.tsv'\n",
    "output_filename = 'out2.tsv'\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8', newline='') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    header = next(reader)  # Read the header row\n",
    "\n",
    "    # Find the indices of the desired columns\n",
    "    column_indices = [header.index(column) for column in ['COMPANY', 'CITY', 'STATE', 'FORTUNE', 'FORBES', 'AMLAW', '2008_CEI', '2006_CEI']]\n",
    "    \n",
    "    with open(output_filename, 'w', encoding='utf-8', newline='') as output_file:\n",
    "        writer = csv.writer(output_file, delimiter='\\t')\n",
    "        writer.writerow(header + ['ERROR_NOTE'])  # Write the header row with the error note\n",
    "\n",
    "        for row in reader:\n",
    "            data = []\n",
    "\n",
    "            # Extract the values for the desired columns\n",
    "            for index in column_indices:\n",
    "                if index < len(row):\n",
    "                    data.append(row[index])\n",
    "                else:\n",
    "                    data.append('')\n",
    "            \n",
    "            if len(row) < len(header):\n",
    "                data.append('##ERROR_HERE_123!')\n",
    "            \n",
    "            writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def parse_2008_tsv():\n",
    "    with open('2008.tsv', 'r') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        rows = list(reader)\n",
    "\n",
    "    with open('out2.tsv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter='\\t')\n",
    "\n",
    "        for row in rows:\n",
    "            state_abbrev = row[2]\n",
    "            last_item = row[-1]\n",
    "            if state_abbrev != 'nan' and last_item.endswith('nan') and any(isinstance(item, float) and item.is_integer() for item in row[3:-1]):\n",
    "                row = row[:7]  # Keep the first 7 items (indices 0 to 6)\n",
    "            else:\n",
    "                row = row[:8]  # Keep the first 8 items (indices 0 to 7)\n",
    "            writer.writerow(row)\n",
    "\n",
    "parse_2008_tsv()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
